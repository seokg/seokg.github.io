<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script> -->

  <!-- Google Tag Manager -->
  <!-- <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5KCXXNS');</script> -->
  <!-- End Google Tag Manager -->

  <title>Kwanggyoon Seo</title>

  <meta name="author" content="Kwanggyoon Seo">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- add favicon -->
  <link rel="icon" type="image/png" href="assets/logo/VML_Logo.png">
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5KCXXNS" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Kwanggyoon Edward Seo</name>
                  </p>
                  <p>
                    Hello World! I am a PhD candiate at <a href="https://ct.kaist.ac.kr/">GCST</a> <a
                      href="https://www.kaist.ac.kr/en/">KAIST</a> advised by Prof. <a
                      href="https://scholar.google.com/citations?user=u75_aBgAAAAJ&hl=en">Junyong Noh</a>. 
                       During my PhD, I was fortunate to work at Adobe Research with wonderful mentors <a href="https://sites.google.com/view/seoungwugoh">Seoung Wug Oh</a>,
                      <a href="https://joonyoung-cv.github.io/">Joon-Young Lee</a>, and <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan (Cynthia) Lu</a>.
                       Also, I spent time at Naver Clova working with Suntae Kim and Soonmin Bae. 
                  </p>
                  <p>
                    My research lies at the intersection of deep learning, computer vision, and computer graphics. Specifically, I am interests in generative AI focusing on manipulating images/videos and animating 3D human.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:seokg1023@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/cv_2024.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=AQt43oYAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/seokg">Github</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/seokg/">LinkedIn</a>
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://twitter.com/edwardoseo">Twitter</a> -->
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="assets/seokg01.png"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="assets/seokg01.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Research</heading>
                  <!-- <p>
                I'm interested in computer vision, machine learning, optimization, and image processing.
                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.
                Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              
              <!-- MAAFA -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <video poster="" autoplay muted loop width="256">
                    <source src="assets/research/vid/seo24maafa.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Mesh-Agnostic Audio-Driven 3D Facial Animation</papertitle>
                  <br>
                  <myname>Kwanggyoon Seo*</myname>,
                  <a href="https://scholar.google.co.kr/citations?user=gwSV7H4AAAAJ&hl=ko">Sihun Cha*</a>,
                  <a href="https://vml.kaist.ac.kr/main/people">Hyeonho Na</a>,
                  <a href="https://vml.kaist.ac.kr/main/people">Inyup Lee</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  In submission
                  <br>
                  <!-- <a href="">paper</a> / -->
                  <!-- <a href="">page</a> / -->
                  <!-- <a href="">code</a> -->
                  <p></p>
                  <p>An end-to-end approach to animating a 3D face mesh with arbitrary shape and triangulation from a given speech audio.</p>
                </td>
              </tr>


              <!-- LMEDIT -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <!-- <img src="https://style-portrait-video.github.io/assets/edo-00.mp4" alt="PontTuset" width="256" style="border-style: none"> -->
                  <video poster="" autoplay muted loop width="256">
                    <source src="assets/research/vid/seo24lmedit_3.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <papertitle>LMEDIT: Emotion Manipulation for Talking-head Videos via Facial Landmarks</papertitle> -->
                  <papertitle>Emotion Manipulation for Talking-head Videos</papertitle>
                  <br>
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://renejotham-dev.vercel.app/">Rene Jotham Culaway</a>,
                  <a href="https://sites.google.com/view/bulee">Byeong-Uk Lee</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  In submission
                  <br>
                  <!-- <a href="">paper</a> / -->
                  <!-- <a href="https://lmedit.github.io/">page</a> / -->
                  <!-- <a href="">code</a> -->
                  <p></p>
                  <p>Propose a latent-based landmark detection and latent manipulation module to edit the emotion of portrait video that faithfully follows the original lip-synchronization or lip-contact.</p>
                </td>
              </tr>


              <!-- Audio2Style -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <!-- <img src="https://style-portrait-video.github.io/assets/edo-00.mp4" alt="PontTuset" width="256" style="border-style: none"> -->
                  <video poster="" autoplay muted loop width="256">
                    <source src="assets/research/vid/seo24audio2style.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <papertitle>Transferring Latent Motion for Audio-Driven Emotional Talking-Head Generation</papertitle> -->
                  <papertitle>Audio-Driven Emotional Talking-Head Generation</papertitle>
                  <br>
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://kwanyun.github.io">Kwan Yun</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/18">Sunjin Jung</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  In submission
                  <br>
                  <!-- <a href="">paper</a> /-->
                  <!-- <a href="https://audio2style.github.io/">page</a> / -->
                  <!-- <a href="">code</a> -->
                  <p></p>
                  <p>Use a generative prior for identity agnostic audio-driven talking-head generation with emotion manipulation while trained on a single identity audio-visual dataset.</p>
                </td>
              </tr>
              <!-- Speed-Aware Audio2Face -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/jung24speed.jpg" alt="PontTuset" width="256" style="border-style: none">
                </td>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Speed-Aware Audio-Driven Speech Animation using Adaptive Windows</papertitle>
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/people/person/18">Sunjin Jung</a>,
                  <a href="https://blog.naver.com/seolzzang01/130169855593">Yeongho Seol</a>,
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://vml.kaist.ac.kr/main/people/">Hyeonho Na</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/139">Seonghyeon Kim</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/">Vanessa Tan</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  minor revision to TOG 2024
                  <br>
                  <!-- <a href="">paper</a> /
                  <a href="">page</a> /
                  <a href="">code</a> -->
                  <p></p>
                  <p> A novel method that can generate realistic speech animations of a 3D face from audio using multiple adaptive windows.</p>
                </td>
              </tr>
              <!-- LeGO -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/yoon24lego.png" alt="PontTuset" width="256" style="border-style: none">
                </td>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example</papertitle>
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/people">Soyeon Yoon*</a>,
                  <a href="https://kwanyun.github.io">Kwan Yun*</a>,
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://scholar.google.co.kr/citations?user=gwSV7H4AAAAJ&hl=ko">Sihun Cha</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/19">JungEun Yoo</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  CVPR 2024
                  <br>
                   <a href="https://arxiv.org/abs/2403.15227">paper</a> /
                  <a href="https://kwanyun.github.io/lego/">page</a> /
                  <a href="https://github.com/kwanyun/LeGO_code">code</a>
                  <p></p>
                  <p>Generate stylized 3D face model with a single example paired mesh by finetuning a pre-trained surface deformation network.</p>
                </td>
              </tr>
              
              <!-- StyleCineGAN -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <!-- <img src="https://style-portrait-video.github.io/assets/edo-00.mp4" alt="PontTuset" width="256" style="border-style: none"> -->
                  <video poster="" autoplay muted loop width="256">
                    <source src="assets/research/vid/choi24cine.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN</papertitle>
                  <br>
                  <a href="https://jeolpyeoni.github.io">Jongwoo Choi</a>,
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://scholar.google.com/citations?user=IdMPKBEAAAAJ&hl">Amirsaman Ashtari</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  CVPR 2024
                  <br>
                   <a href="https://arxiv.org/abs/2403.14186">paper</a> /
                  <a href="https://jeolpyeoni.github.io/stylecinegan_project/">page</a> /
                  <a href="https://github.com/jeolpyeoni/StyleCineGAN">code</a>
                  <p></p>
                  <p>Splatting deep generated features of pre-trained StyleGAN for cinemagraph generation.</p>
                </td>
              </tr>




              <!-- StyleSketch -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/yun24stylesketch.png" alt="PontTuset" width="256" style="border-style: none">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>StyleSketch: Stylized Sketch Extraction via Generative Prior with Limited Data</papertitle>
                  <br>
                  <a href="https://reminder-by-kwan.tistory.com/">Kwan Yun*</a>,
                  <myname>Kwanggyoon Seo*</myname>,
                  <a href="https://scholar.google.com/citations?user=DyxGxDMAAAAJ&hl=en">Changwook Seo*</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/177">Soyeon Yoon</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/181">Seongcheol Kim</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/179">Soohyun Ji</a>,
                  <a href="https://scholar.google.com/citations?user=IdMPKBEAAAAJ&hl">Amirsaman Ashtari</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  Eurographics 2024; CGF 2024
                  <br>
                  <a href="https://arxiv.org/abs/2403.11263">paper</a> /
                  <a href="https://kwanyun.github.io/stylesketch_project/">page</a> /
                  <a href="https://github.com/kwanyun/StyleSketch/">code</a> /
                  <a href="https://github.com/kwanyun/SKSF-A/">data</a>
                  <p></p>
                  <p>Train a sketch generator with generated deep features of pre-trained StyleGAN to generate high-quality sketch images with limited data.</p>
                </td>
              </tr>



              <!-- PG StyleVideoPortrait -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <!-- <img src="https://style-portrait-video.github.io/assets/edo-00.mp4" alt="PontTuset" width="256" style="border-style: none"> -->
                  <video poster="" autoplay muted loop width="256">
                      <source src="assets/research/vid/seo22spv.mp4" type="video/mp4">
                  </video>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>StylePortraitVideo: Editing Portrait Videos with Expression Optimization </papertitle>
                  <br>
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://sites.google.com/view/seoungwugoh">Seoung Wug Oh</a>,
                  <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan (Cynthia) Lu</a>,
                  <a href="https://joonyoung-cv.github.io/">Joon-Young Lee</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/139">Seonghyeon Kim</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  Pacific Graphics 2022; CGF 2022
                  <br>
                  <a href="https://diglib.eg.org/handle/10.1111/cgf14666">paper</a> /
                  <a href="https://style-portrait-video.github.io">page</a> /
                  <a href="https://github.com/seokg/spv">code</a>
                  <p></p>
                  <p>A method to edit portrait video using a pre-trained StyleGAN using the video adaptaion
                    and expression dynamics optimization.</p>
                </td>
              </tr>

              <!-- PG Retargeting -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/kim21retarget.jpg" alt="PontTuset" width="256" style="border-style: none">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Deep Learning-Based Unsupervised Human Facial Retargeting</papertitle>
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/people/person/139">Seonghyeon Kim</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/18">Sunjin Jung</a>,
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/2">Roger Blanco i Ribera</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  Pacific Graphics 2021; CGF 2021
                  <br>
                  <a href="https://diglib.eg.org/handle/10.1111/cgf14400">paper</a>
                  <p></p>
                  <p>A novel unsupervised learning method by reformulating the retargeting of 3D facial
                    blendshape-based animation in the image domain.</p>
                </td>
              </tr>


              <!-- Ubicomp Overthere -->
              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/seo21overthere.jpg" alt="PontTuset" width="256" style="border-style: none">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>Overthere : A Simple and Intuitive Object Registration Method for an Absolute Mid-air Pointing Interface</papertitle>
                  </a>
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/people/person/9">Hyunggoog Seo</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/13">Jaedong Kim</a>,
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/12">Bumki Kim</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  <em>IMWUT</em>, 2021; <em>Ubicomp</em>, 2021
                  <br>
                  <a href="">page</a>
                  <p></p>
                  <p>We introduce Overthere, which allows the user to intuitively register the objects in a smart environment by pointing to each target object a few times.</p>
                </td>
              </tr> -->




              <!-- CHI Camera Work -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/yoo21camera.png" alt="PontTuset" width="256" style="border-style: none">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href=""> -->
                  <papertitle>Virtual Camera Layout Generation using a Reference Video</papertitle>
                  <!-- </a> -->
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/people/person/19">JungEun Yoo*</a>,
                  <myname>Kwanggyoon Seo*</myname>,
                  <a href="https://sanghunpark.github.io/resume/">Sanghun Park</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/13">Jaedong Kim</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/122">Dawon Lee</a>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  CHI 2021
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/international/individual/173">paper</a> /
                  <a href="https://youtu.be/F4FO_jQ7-N8">video</a> 
                  <p></p>
                  <p>A method that generates a virtual camera layout for both human and stylzed characters of
                    a 3D animation scene by following the cinematic intention of a reference video.</p>
                </td>
              </tr>

              <!-- SIGASIA Morph -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/research/img/park20crossbreed.png" alt="PontTuset" width="256" style="border-style: none">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <!-- <a href="https://arxiv.org/pdf/2009.00905.pdf"> -->
                  <papertitle>Neural Crossbreed: Neural Based Image Metamorphosis</papertitle>
                  <!-- </a> -->
                  <br>
                  <a href="https://sanghunpark.github.io/resume/">Sanghun Park</a>,
                  <myname>Kwanggyoon Seo</myname>,
                  <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
                  <br>
                  SIGGRAPH Asia 2020; TOG 2020
                  <br>
                  <a href="https://arxiv.org/abs/2009.00905">paper</a> /
                  <a href="https://vml.kaist.ac.kr/main/international/individual/171">page</a> /
                  <a href="https://youtu.be/xKXiPJ3hKSc">video</a> /
                  <a href="https://github.com/sanghunpark/neural_crossbreed">code</a>
                  <p></p>
                  <p>A feed-forward neural network that can learn a semantic change of
                    input images in a latent space to create the morphing effect by distilling the information of pre-trained GAN.</p>
                </td>
              </tr>

              <!-- PG Cinematography -->
              <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="assets/img/seo19cinema.jpg" alt="PontTuset" width="256" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2009.00905.pdf">
                <papertitle>Cinematography Generation using a Reference Video</papertitle>
              </a>
              <br>
              <myname>Kwanggyoon Seo</myname>,
              <a href="https://vml.kaist.ac.kr/main/people/person/19">JungEun Yoo</a>,
              <a href="https://sanghunpark.github.io/resume/">Sanghun Park</a>,
              <a href="https://vml.kaist.ac.kr/main/people/person/13">Jaedong Kim</a>,
              <a href="https://vml.kaist.ac.kr/main/people/person/122">Dawon Lee</a>,
              <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
              <br>
              <em>Pacific Graphics Poster</em>, 2019  
              <br>
              <a href="./data/pg2019wip-poster.pdf">poster</a> /
              <a href="https://vml.kaist.ac.kr/main/international/individual/161/">page</a>                     
              <p></p>
              <p>We propose a system to generate cinematography in 3D animation, which mimics the camera work of a reference video.</p>
            </td>
          </tr> -->

            


            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Research Experience</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Visual Media Lab -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/logo/VML_Logo_Text.png" alt="PontTuset" width="256" style="border-style: none">
                <td width="75%" valign="center">
                  <b>Visual Media Lab</b>
                  <br>
                  <em>Research Assistance</em>
                  <br>
                  <em>Dec.2016-Mar.2024</em>
                  <br>
                  <br>
                  <!-- <a href="https://vml.kaist.ac.kr/main/projects/individual/38">project page</a> -->
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Adobe Research -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/logo/adobe.png" alt="PontTuset" width="256" style="border-style: none">
                <td width="75%" valign="center">
                  <b>Adobe Research</b>
                  <br>
                  <em>Research Intern</em>
                  <br>
                  <em>Mar.2021-Jun.2021; Jun.2022-Aug.2022</em>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- NAVER Clova -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/logo/clova_black.png" alt="PontTuset" width="256" style="border-style: none">
                <td width="75%" valign="center">
                  <b>NAVER Corp. </b>
                  <br>
                  <em>Research Intern</em>
                  <br>
                  <em>Dec.2019-Jun.2020</em>
                  <br>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Project -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Project</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- IITP 3D Cinemagraph -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/logo/iitp.png" alt="PontTuset" width="256" style="border-style: none">
                <td width="75%" valign="center">
                  <b>3D Cinemagraph for AR Contents Creation</b> <br>
                  <em>June.2020-Dec.2022</em>
                  <br>
                  <br>
                  Develop user-friendly content production technology that enables general users to easily transform a
                  single image into immersive AR contents where background and characters within the image move and
                  interact with real-world objects.
                  <br>
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/projects/individual/41">project page</a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- KOCCA Camera Work -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="assets/logo/kocca.png" alt="PontTuset" width="256" style="border-style: none">
                <td width="75%" valign="center">
                  <b>Development of Camera Work Tracking Technology for Animation Production using Artificial
                    Intelligence</b> <br>
                  <em>May.2018-Dec.2019</em>
                  <br>
                  <br>
                  Analyze cinematography properties of a reference video clip using neural networks and replicate the
                  cinematic intention of the reference video to the 3D animation.
                  <br>
                  <br>
                  <a href="https://vml.kaist.ac.kr/main/projects/individual/38">project page</a>
                </td>
              </tr>
            </tbody>
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    The source code of this website is from <a href="https://jonbarron.info/">Jon Barron</a>.
                  </p>
                  <p style="text-align:right;font-size:small;">
                    (last update: May.25 2024) 
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        
        </td>
      </tr>
  </table>
</body>

</html>
