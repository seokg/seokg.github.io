<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script> -->

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5KCXXNS');</script>
  <!-- End Google Tag Manager -->
  
  <title>Kwanggyoon Seo</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- add favicon -->
  <link rel="icon" type="image/png" href="images/VML_Logo.png">
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5KCXXNS"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kwanggyoon Edward Seo</name>
              </p>
              <p>I am a PhD candidate at <a href="https://ct.kaist.ac.kr/">GCST</a> <a href="https://www.kaist.ac.kr/en/">KAIST</a>, where I am advised by Prof. <a href="https://scholar.google.com/citations?user=u75_aBgAAAAJ&hl=en">Junyong Noh</a>.
                I am currently a team leader at <a href="https://vml.kaist.ac.kr/">Visual Media Lab.</a>
                <!-- I did my M.S. and B.S. in electrical engineering and GSCT KAIST in 2016 and 2018, respectively. -->
                <!-- <br> -->
                My research interests are in image manipulation, deep learning, and graphics.
                I like to see, learn, and research on visually appealing techniques.
                <!-- <a href="https://vml.kaist.ac.kr">Visual Media Lab</a> -->
              </p>
              <p>

                <!-- At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
                I've recieved the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
              </p>
              <p style="text-align:center">
                <a href="mailto:seokg1023@kaist.ac.kr">Email</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=AQt43oYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/edwardoseo">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/seokg01.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/seokg01.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <!-- <p>
                I'm interested in computer vision, machine learning, optimization, and image processing.
                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.
                Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- SIGASIA Morph -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Park20Crossbreed.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2009.00905.pdf">
                <papertitle>Neural Crossbreed: Neural Based Image Metamorphosis</papertitle>
              </a>
              <br>
              <a href="https://sanghunpark.github.io/resume/">Sanghun Park</a>,
              <strong>Kwanggyoon Seo</strong>,
              <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
              <br>
              <em>SIGGRAPH Asia</em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/2009.00905">arxiv</a> /
              <a href="https://vml.kaist.ac.kr/main/international/individual/171">page</a> /
              <a href="https://github.com/sanghunpark/neural_crossbreed">code</a>
              <p></p>
              <p>We propose Neural Crossbreed, a feed-forward neural network that can learn a semantic change of input images in a latent space to create the morphing effect.</p>
            </td>
          </tr>

          <!-- PG Cinematography -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Seo19Cinema.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2009.00905.pdf">
                <papertitle>Cinematography Generation using a Reference Video</papertitle>
              </a>
              <br>
              <strong>Kwanggyoon Seo</strong>,
              <a href="https://vml.kaist.ac.kr/main/people/person/19">JungEun Yoo</a>,
              <a href="https://sanghunpark.github.io/resume/">Sanghun Park</a>,
              <a href="https://vml.kaist.ac.kr/main/people/person/13">Jaedong Kim</a>,
              <a href="https://vml.kaist.ac.kr/main/people/person/122">Dawon Lee</a>,
              <a href="https://vml.kaist.ac.kr/main/people/person/1">Junyong Noh</a>
              <br>
              <em>Pacific Graphics Poster</em>, 2019  
              <br>
              <a href="./data/pg2019wip-poster.pdf">poster</a> /
              <a href="https://vml.kaist.ac.kr/main/international/individual/161/">project page</a>                     
              <p></p>
              <p>We propose a system to generate cinematography in 3D animation, which mimics the camera work of a reference video.</p>
            </td>
          </tr>

          <!-- <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nlt_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nlt_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nlt_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nlt_start() {
                  document.getElementById('nlt_image').style.opacity = "1";
                }

                function nlt_stop() {
                  document.getElementById('nlt_image').style.opacity = "0";
                }
                nlt_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://nlt.csail.mit.edu/">
                <papertitle>Neural Light Transport for Relighting and View Synthesis</papertitle>
              </a>
              <br>
              <a href="http://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
              <a href="http://www.seanfanello.it/">Sean Fanello</a>,
              <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
              <a href="http://kevinkingo.com/">Tiancheng Sun</a>,
              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,
              <a href="https://research.google/people/106687/">Rohit Pandey</a>,
              <a href="https://www.dtic.ua.es/~sorts/">Sergio Orts-Escolano</a>,
              <a href="https://dl.acm.org/profile/99659224296">Philip Davidson</a>,
              <a href="https://scholar.google.com/citations?user=5D0_pjcAAAAJ&hl=en">Christoph Rhemann</a>,
              <a href="http://www.pauldebevec.com/">Paul Debevec</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="http://billf.mit.edu/">William T. Freeman</a>
              <br>
              <em>arXiv</em>, 2020  
              <br>
              <a href="http://nlt.csail.mit.edu/">project page</a> /
              <a href="https://arxiv.org/abs/2008.03806">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=OGEnCWZihHE">video</a>
              <p></p>
              <p>Embedding a convnet within a predefined texture atlas enables simultaneous view synthesis and relighting.</p>
            </td>
          </tr>   

          <tr onmouseout="nerfw_stop()" onmouseover="nerfw_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfw_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfw_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfw_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function nerfw_start() {
                  document.getElementById('nerfw_image').style.opacity = "1";
                }

                function nerfw_stop() {
                  document.getElementById('nerfw_image').style.opacity = "0";
                }
                nerfw_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nerf-w.github.io/">
                <papertitle>NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</papertitle>
              </a>
              <br>
              <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla*</a>,
              <a href="https://scholar.google.com/citations?user=g98QcZUAAAAJ&hl=en">Noha Radwan*</a>,
              <a href="https://research.google/people/105804/">Mehdi S. M. Sajjadi*</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://scholar.google.com/citations?user=FXNJRDoAAAAJ&hl=en">Alexey Dosovitskiy</a>,
              <a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth</a>
              <br>
              <em>arXiv</em>, 2020  
              <br>
              <a href="https://nerf-w.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2008.02268">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=yPKIxoN2Vf0">video</a>
              <p></p>
              <p>Letting NeRF reason about occluders and appearance variation produces photorealistic view synthesis using only unstructured internet photos.</p>
            </td>
          </tr> 

          <tr onmouseout="ff_stop()" onmouseover="ff_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ff_image'>
                  <img src='images/lion_ff.jpg' width="160"></div>
                <img src='images/lion_none.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ff_start() {
                  document.getElementById('ff_image').style.opacity = "1";
                }

                function ff_stop() {
                  document.getElementById('ff_image').style.opacity = "0";
                }
                ff_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://people.eecs.berkeley.edu/~bmild/fourfeat/index.html">
                <papertitle>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</papertitle>
              </a>
              <br>
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan*</a>,
              <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>,
              <a href="https://www.linkedin.com/in/nithinraghavan">Nithin Raghavan</a>,
              <a href="https://scholar.google.com/citations?user=lvA86MYAAAAJ&hl=en">Utkarsh Singhal</a>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>NeurIPS</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://people.eecs.berkeley.edu/~bmild/fourfeat/">project page</a> /
              <a href="https://arxiv.org/abs/2006.10739">arXiv</a> /
              <a href="https://github.com/tancik/fourier-feature-networks">code</a>
              <p></p>
              <p>Composing neural networks with a simple Fourier feature mapping allows them to learn detailed high-frequency functions.</p>
            </td>
          </tr> 

    
          <tr onmouseout="thresh_stop()" onmouseover="thresh_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='thresh_image'>
                  <img src='images/thresh_after.png' width="160"></div>
                <img src='images/thresh_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function thresh_start() {
                  document.getElementById('thresh_image').style.opacity = "1";
                }

                function thresh_stop() {
                  document.getElementById('thresh_image').style.opacity = "0";
                }
                thresh_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.07350">
                <papertitle>A Generalization of Otsu's Method and Minimum Error Thresholding</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>ECCV</em>, 2020 &nbsp <font color=#FF8080><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://github.com/jonbarron/hist_thresh">code</a> / 
              <a href="https://www.youtube.com/watch?v=rHtQQlQo1Q4">video</a> / 
              <a href="data/BarronECCV2020.bib">bibtex</a>
              <br>
              <p></p>
              <p>
              A simple and fast Bayesian algorithm that can be written in ~10 lines of code outperforms or matches giant CNNs on image binarization, and unifies three classic thresholding algorithms.
              </p>
            </td>
          </tr>  
    
    
          <tr onmouseout="uflow_stop()" onmouseover="uflow_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='uflow_image'>
                  <img src='images/uflow_after.png' width="160"></div>
                <img src='images/uflow_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function uflow_start() {
                  document.getElementById('uflow_image').style.opacity = "1";
                }

                function uflow_stop() {
                  document.getElementById('uflow_image').style.opacity = "0";
                }
                uflow_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2006.04902">
                <papertitle>What Matters in Unsupervised Optical Flow</papertitle>
              </a>
              <br>
              <a href="http://ricojonschkowski.com/">Rico Jonschkowski</a>,
              <a href="https://www.linkedin.com/in/austin-charles-stone-1ba33b138/">Austin Stone</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="https://research.google/people/ArielGordon/">Ariel Gordon</a>,
              <a href="https://www.linkedin.com/in/kurt-konolige/">Kurt Konolige</a>,
              <a href="https://research.google/people/AneliaAngelova/">Anelia Angelova</a>
              <br>
              <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://github.com/google-research/google-research/tree/master/uflow">code</a>
              <br>
              <p></p>
              <p>
              Extensive experimentation yields a simple optical flow technique that is trained on only unlabeled videos, but still works as well as supervised techniques.
              </p>
            </td>
          </tr>  
    
          <tr onmouseout="nerf_stop()" onmouseover="nerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/vase_small.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/vase_still.png' width="160">
              </div>
              <script type="text/javascript">
                function nerf_start() {
                  document.getElementById('nerf_image').style.opacity = "1";
                }

                function nerf_stop() {
                  document.getElementById('nerf_image').style.opacity = "0";
                }
                nerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/nerf">
                <papertitle>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</papertitle>
              </a>
              <br>
              <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall*</a>,
              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan*</a>,
              <a href="http://matthewtancik.com/">Matthew Tancik*</a>,
              <strong>Jonathan T. Barron</strong>,
              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>
              <br>
              <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
              <br>
              <a href="http://www.matthewtancik.com/nerf">project page</a>
              /
              <a href="https://arxiv.org/abs/2003.08934">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=LRAqeM8EjOo&t">talk video</a>
              /
              <a href="https://www.youtube.com/watch?v=JuH79E8rdKc">supp video</a>
              /
              <a href="https://github.com/bmild/nerf">code</a>
              <p></p>
              <p>
              Training a tiny non-convolutional neural network to reproduce a scene using volume rendering achieves photorealistic view synthesis.</p>
            </td>
          </tr>  -->


        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Research Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <!-- Visual Media Lab -->
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/VML_Logo_Text.png" alt="PontTuset" width="160" style="border-style: none">
            <td width="75%" valign="center">
              <b>Visual Media Lab</b> 
              <br>
              <em>Research Assistance</em>
              <br>
              <em>Dec.2016-current</em>
              <br>
              <br>
              <!-- <a href="https://vml.kaist.ac.kr/main/projects/individual/38">project page</a> -->
            </td>
          </tr>
        </tbody></table>
        <!-- NAVER Clova -->
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/clova_black.png" alt="PontTuset" width="160" style="border-style: none">
            <td width="75%" valign="center">
              <b>NAVER Corp. </b>
              <br>
              <em>Research Intern at Clova</em>
              <br>
              <em>Dec.2019-June.2020</em>
              <br>
            </td>
          </tr>
        </tbody></table>
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Project</heading>
            </td>
          </tr>
        </tbody></table>
        <!-- IITP 3D Cinemagraph -->
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iitp.png" alt="PontTuset" width="160" style="border-style: none">
            <td width="75%" valign="center">
              <b>3D Cinemagraph for AR Contents Creation</b> <br>
              <em>June.2020-Current</em>
              <br>
              <br>
              <!-- Analyze cinematography properties of video clips using neural networks and replicate the cinematographic cameraposition and movement in 3D animation. -->
              <br>
              <br>
              <a href="">project page</a>
              <!-- <a href="https://vml.kaist.ac.kr/main/projects/individual/38">project page</a> -->
            </td>
          </tr>
        </tbody></table>
        <!-- KOCCA Camera Work -->
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kocca.png" alt="PontTuset" width="160" style="border-style: none">
            <td width="75%" valign="center">
              <b>Development of Camera Work Tracking Technology for Animation Production using Artificial Intelligence</b> <br>
              <em>May.2018-Dec.2019</em>
              <br>
              <br>
              Analyze cinematography properties of a reference video clip using neural networks and replicate the cinematic intention of the reference video to the 3D animation.
              <!-- The goal of this project is to learn the cinematography of live-action movies using artificial inteligence techniques and recreate the camera work so that it can be used for animation production. -->
              <br>
              <br>
              <a href="https://vml.kaist.ac.kr/main/projects/individual/38">project page</a>
            </td>
          </tr>
        </tbody></table>
      
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                The source code of this website is from <a href="https://jonbarron.info/">Jon Barron</a>. 
              </p>
              <p style="text-align:right;font-size:small;">
                <!-- (last update: Sep-28 2020)  -->
              </p>
            </td>
          </tr>
        </tbody></table> 
      </td>
    </tr>
  </table>
</body>

</html>



